import os
from typing import Optional, Dict, Any, List
import requests
import concurrent.futures
from src.utils.logger import log


class SearchService:
    """SerpAPIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        youtube_api_key: Optional[str] = None,
    ):
        """
        Args:
            api_key: SerpAPI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
            youtube_api_key: YouTube Data API v3 í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        """
        self.api_key = api_key or os.getenv("SERPAPI_API_KEY")
        if not self.api_key:
            raise ValueError("SERPAPI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.youtube_api_key = youtube_api_key or os.getenv("YOUTUBE_API_KEY")
        if not self.youtube_api_key:
            log(self.__class__.__name__, "[Warning] YOUTUBE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. YouTube ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

        self.base_url = "https://serpapi.com/search"
        self.youtube_base_url = "https://www.googleapis.com/youtube/v3/search"

    def search(self, query: str, num_results: int = 3) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ì–´ë¡œ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ì–´
            num_results: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ 3ê°œ)

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        log(self.__class__.__name__, f"\nê²€ìƒ‰ ì‹œì‘")
        log(self.__class__.__name__, f"query: {query}")
        log(self.__class__.__name__, f"num_results: {num_results}")

        try:
            params = {
                "q": query,
                "api_key": self.api_key,
                "num": num_results,
                "hl": "ko",  # í•œêµ­ì–´ ê²°ê³¼
                "gl": "kr",  # í•œêµ­ ì§€ì—­
            }

            response = requests.get(self.base_url, params=params, timeout=10)
            response.raise_for_status()

            result = response.json()

            log(self.__class__.__name__, f"SerpAPI ì‘ë‹µ keys: {list(result.keys())}")

            if "organic_results" in result:
                organic_count = len(result["organic_results"])
                log(self.__class__.__name__, f"organic_results ê°œìˆ˜: {organic_count}")

                if organic_count > 0:
                    log(self.__class__.__name__, f"ìƒìœ„ ê²°ê³¼ ì œëª©:")
                    for i, item in enumerate(result["organic_results"][:3], 1):
                        title = item.get("title", "")
                        log(self.__class__.__name__, f"  [{i}] {title}")

            return result

        except requests.exceptions.RequestException as e:
            log(self.__class__.__name__, f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}

    def extract_summary(self, search_results: Dict[str, Any]) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.
        """
        if "error" in search_results:
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {search_results['error']}"

        if "answer_box" in search_results:
            answer = search_results["answer_box"]
            if "answer" in answer:
                return f"âœ“ {answer['answer']}"
            elif "snippet" in answer:
                return f"âœ“ {answer['snippet']}"

        if "knowledge_graph" in search_results:
            kg = search_results["knowledge_graph"]
            summary = []
            if "title" in kg:
                summary.append(f"ğŸ“Œ {kg['title']}")
            if "description" in kg:
                summary.append(kg["description"])
            if summary:
                return "\n".join(summary)

        if "organic_results" in search_results and search_results["organic_results"]:
            results = []
            for result in search_results["organic_results"][:3]:
                title = result.get("title", "")
                snippet = result.get("snippet", "")
                date = result.get("date", "")

                if snippet:
                    if date:
                        results.append(f"â€¢ [{date}] {snippet}")
                    else:
                        results.append(f"â€¢ {snippet}")

            if results:
                return "\n".join(results)

        return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def _get_youtube_time_filter(self, tbs_value: Optional[str]) -> Optional[str]:
        """
        Google tbs ê°’ì„ YouTube Data API v3ì˜ publishedAfter ë‚ ì§œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        if not tbs_value:
            return None

        from datetime import datetime, timedelta

        now = datetime.utcnow()

        if tbs_value == "qdr:m3":
            published_after = now - timedelta(days=90)
        elif tbs_value == "qdr:m6":
            published_after = now - timedelta(days=180)
        elif tbs_value == "qdr:y":
            published_after = now - timedelta(days=365)
        else:
            return None

        return published_after.strftime("%Y-%m-%dT%H:%M:%SZ")

    def _search_google_images(self, query: str, tbs_value: Optional[str]) -> List[Dict[str, Any]]:
        """
        Google Imagesì—ì„œ SNS ì½˜í…ì¸ ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        params = {
            "q": query,
            "api_key": self.api_key,
            "engine": "google_images",
            "num": 50,
            "hl": "ko",
            "gl": "kr",
        }
        if tbs_value:
            params["tbs"] = tbs_value

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            results = response.json()

            candidates = []
            if "images_results" in results:
                for result in results["images_results"]:
                    link = result.get("link", "")

                    if "instagram.com" in link and ("/p/" in link or "/reel/" in link):
                        candidates.append({"platform": "instagram", "url": link, "thumbnail": result.get("thumbnail") or result.get("original", ""), "title": result.get("title") or result.get("source", ""), "source": "google_images"})
                    elif "youtube.com/watch" in link or "youtu.be/" in link:
                        candidates.append({"platform": "youtube", "url": link, "thumbnail": result.get("thumbnail") or result.get("original", ""), "title": result.get("title") or result.get("source", ""), "source": "google_images"})

                    if len(candidates) >= 5:
                        break
            return candidates
        except Exception as e:
            log(self.__class__.__name__, f"Google Images ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _search_youtube_direct(self, query: str, published_after: Optional[str]) -> List[Dict[str, Any]]:
        """
        YouTube Data API v3ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        if not self.youtube_api_key:
            log(self.__class__.__name__, "YouTube API í‚¤ê°€ ì—†ì–´ ì§ì ‘ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return []

        params = {
            "part": "snippet",
            "q": query,
            "key": self.youtube_api_key,
            "type": "video",
            "maxResults": 5,
            "regionCode": "KR",
            "relevanceLanguage": "ko",
            "order": "relevance",
        }
        if published_after:
            params["publishedAfter"] = published_after

        try:
            log(self.__class__.__name__, f"YouTube Direct ê²€ìƒ‰ì–´: {query}, publishedAfter={published_after or 'None'}")
            response = requests.get(self.youtube_base_url, params=params, timeout=30)
            response.raise_for_status()
            results = response.json()
            log(self.__class__.__name__, f"YouTube Direct API ì‘ë‹µ keys: {results.keys()}")

            candidates = []
            if "items" in results:
                log(self.__class__.__name__, f"YouTube Direct items ê°œìˆ˜: {len(results['items'])}")
                for idx, item in enumerate(results["items"]):
                    video_id = item.get("id", {}).get("videoId", "")
                    snippet = item.get("snippet", {})
                    title = snippet.get("title", "")
                    thumbnail = snippet.get("thumbnails", {}).get("high", {}).get("url", "")
                    log(self.__class__.__name__, f"YouTube Direct #{idx}: {title[:50]}...")
                    if video_id:
                        url = f"https://www.youtube.com/watch?v={video_id}"
                        candidates.append({"platform": "youtube", "url": url, "thumbnail": thumbnail, "title": title, "source": "youtube_api_v3"})
            log(self.__class__.__name__, f"YouTube Direct í›„ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(candidates)}ê°œ")
            return candidates
        except Exception as e:
            log(self.__class__.__name__, f"YouTube Direct ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def search_sns_content(self, query: str, user_question: str = "", relevance_checker=None, has_recency_keyword: bool = True) -> Dict[str, Any]:
        """
        SNS ì½˜í…ì¸ (Instagram, YouTube ë“±)ë¥¼ ê²€ìƒ‰í•˜ê³  ë§í¬ì™€ ì¸ë„¤ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        time_ranges = [("qdr:m3", "ìµœê·¼ 3ê°œì›”"), ("qdr:m6", "ìµœê·¼ 6ê°œì›”"), ("qdr:y", "ìµœê·¼ 1ë…„"), (None, "ì „ì²´ ê¸°ê°„")]
        log(self.__class__.__name__, f"\nSNS ê²€ìƒ‰ ì‹œì‘")
        log(self.__class__.__name__, f"query: {query}")
        time_filter_desc = [f"{tbs or 'None'} ({name})" for tbs, name in time_ranges]
        log(self.__class__.__name__, f"time_filters: {time_filter_desc}")

        for idx, (tbs_value, period_name) in enumerate(time_ranges, 1):
            try:
                log(self.__class__.__name__, f"\nSNS ê²€ìƒ‰ [{idx}/{len(time_ranges)}] ì‹œë„: {period_name} (tbs={tbs_value or 'None'})")
                log(self.__class__.__name__, f"SNS ê²€ìƒ‰ Debug - ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: tbs={tbs_value or 'None'} ({period_name})")

                published_after = self._get_youtube_time_filter(tbs_value)
                youtube_query = query
                remove_keywords = ["ìœ íŠœë¸Œ", "youtube", "ì˜ìƒ", "ë™ì˜ìƒ", "ë¹„ë””ì˜¤", "video"]
                for keyword in remove_keywords:
                    youtube_query = youtube_query.replace(keyword, "").replace(keyword.upper(), "").replace(keyword.capitalize(), "")
                youtube_query = " ".join(youtube_query.split())

                if youtube_query != query:
                    log(self.__class__.__name__, f"YouTube Query ê²€ìƒ‰ì–´ ê°•í™”: '{query}' â†’ '{youtube_query}'")

                with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                    images_future = executor.submit(self._search_google_images, query, tbs_value)
                    youtube_future = executor.submit(self._search_youtube_direct, youtube_query, published_after)
                    images_candidates = images_future.result()
                    youtube_candidates = youtube_future.result()

                log(self.__class__.__name__, f"SNS ê²€ìƒ‰ Debug - Google Images í›„ë³´: {len(images_candidates)}ê°œ")
                log(self.__class__.__name__, f"SNS ê²€ìƒ‰ Debug - YouTube Direct í›„ë³´: {len(youtube_candidates)}ê°œ")

                all_candidates = youtube_candidates + images_candidates
                seen_urls = set()
                unique_candidates = []
                for candidate in all_candidates:
                    url = candidate.get("url", "")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        unique_candidates.append(candidate)

                log(self.__class__.__name__, f"SNS ê²€ìƒ‰ Debug - ë³‘í•© í›„ ê³ ìœ  í›„ë³´: {len(unique_candidates)}ê°œ")

                for idx, candidate in enumerate(unique_candidates):
                    url = candidate.get("url", "")
                    platform = candidate.get("platform", "")
                    title = candidate.get("title", "")
                    thumbnail = candidate.get("thumbnail", "")
                    source = candidate.get("source", "")
                    log(self.__class__.__name__, f"SNS ê²€ìƒ‰ Debug - Candidate #{idx}: platform={platform}, url={url[:80] if url else 'None'}, source={source}")

                    if relevance_checker and user_question:
                        is_relevant = relevance_checker.act(user_question=user_question, sns_title=title, platform=platform, search_term=query)
                        if is_relevant:
                            log(self.__class__.__name__, f"SNS ê²€ìƒ‰ Debug - âœ… ê´€ë ¨ì„± í™•ì¸ ì™„ë£Œ ({period_name}) â†’ ê²°ê³¼ ë°˜í™˜")
                            return {"found": True, "platform": platform, "url": url, "thumbnail": thumbnail, "title": title}
                        else:
                            log(self.__class__.__name__, f"SNS ê²€ìƒ‰ Debug - âŒ ê´€ë ¨ì„± ì—†ìŒ â†’ ë‹¤ìŒ ê²°ê³¼ íƒìƒ‰")
                            continue
                    else:
                        log(self.__class__.__name__, f"SNS ê²€ìƒ‰ Debug - âœ… ê´€ë ¨ì„± ê²€ì¦ ì—†ì´ ê²°ê³¼ ë°˜í™˜ ({period_name})")
                        return {"found": True, "platform": platform, "url": url, "thumbnail": thumbnail, "title": title}

                log(self.__class__.__name__, f"SNS ê²€ìƒ‰ Debug - âŒ {period_name}ì—ì„œ SNS ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨ â†’ Fallback ì‹œë„")
                if idx < len(time_ranges):
                    next_period = time_ranges[idx][1]
                    log(self.__class__.__name__, f"ë‹¤ìŒ ì‹œë„: {next_period}")

            except Exception as e:
                import traceback
                log(self.__class__.__name__, f"SNS ê²€ìƒ‰ ì˜¤ë¥˜ ({period_name}): {e}")
                log(self.__class__.__name__, f"Traceback:\n{traceback.format_exc()}")
                if idx < len(time_ranges):
                    next_period = time_ranges[idx][1]
                    log(self.__class__.__name__, f"ì˜¤ë¥˜ ë°œìƒ - ë‹¤ìŒ ì‹œë„: {next_period}")
                continue

        log(self.__class__.__name__, f"\nSNS ê²€ìƒ‰ âŒ ìµœì¢… ê²°ê³¼: ëª¨ë“  ì‹œê°„ ë²”ìœ„({len(time_ranges)}ê°œ)ì—ì„œ SNS ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨")
        return {"found": False}