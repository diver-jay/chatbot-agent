import os
from typing import Optional, Dict, Any, List
import requests
import concurrent.futures
from src.utils.logger import log


class SearchService:
    def __init__(
        self,
        api_key: Optional[str] = None,
        youtube_api_key: Optional[str] = None,
    ):
        self.api_key = api_key or os.getenv("SERPAPI_API_KEY")
        if not self.api_key:
            raise ValueError("SERPAPI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.youtube_api_key = youtube_api_key or os.getenv("YOUTUBE_API_KEY")
        if not self.youtube_api_key:
            log(
                self.__class__.__name__,
                "[Warning] YOUTUBE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. YouTube ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.",
            )

        self.base_url = "https://serpapi.com/search"
        self.youtube_base_url = "https://www.googleapis.com/youtube/v3/search"

    def search_web(self, query: str, num_results: int = 3) -> Dict[str, Any]:
        log(self.__class__.__name__, f"\nê²€ìƒ‰ ì‹œìž‘")
        log(self.__class__.__name__, f"query: {query}")
        log(self.__class__.__name__, f"num_results: {num_results}")

        try:
            params = {
                "q": query,
                "api_key": self.api_key,
                "num": num_results,
                "hl": "ko",  # í•œêµ­ì–´ ê²°ê³¼
                "gl": "kr",  # í•œêµ­ ì§€ì—­
            }

            response = requests.get(self.base_url, params=params, timeout=10)
            response.raise_for_status()

            result = response.json()

            log(self.__class__.__name__, f"SerpAPI ì‘ë‹µ keys: {list(result.keys())}")

            if "organic_results" in result:
                log(
                    self.__class__.__name__,
                    f"organic_results ê°œìˆ˜: {len(result['organic_results'])}",
                )

                if len(result["organic_results"]) > 0:
                    log(self.__class__.__name__, f"ìƒìœ„ ê²°ê³¼ ì œëª©:")
                    for i, item in enumerate(result["organic_results"][:3], 1):
                        log(self.__class__.__name__, f"  [{i}] {item.get('title', '')}")

            return result

        except requests.exceptions.RequestException as e:
            log(self.__class__.__name__, f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}

    def search_sns_content(
        self,
        query: str,
        user_question: str = "",
        relevance_checker=None,
    ) -> Dict[str, Any]:
        """
        SNS ì½˜í…ì¸ (Instagram, YouTube ë“±)ë¥¼ ê²€ìƒ‰í•˜ê³  ë§í¬ì™€ ì¸ë„¤ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ëª¨ë“  ì‹œê°„ ë²”ìœ„ë¥¼ ë³‘ë ¬ë¡œ ê²€ìƒ‰í•˜ê³ , ìµœì‹  ì½˜í…ì¸ ë¶€í„° ìš°ì„ ìˆœìœ„í™”í•©ë‹ˆë‹¤.
        """
        time_ranges = [
            ("qdr:m3", "ìµœê·¼ 3ê°œì›”"),
            ("qdr:m6", "ìµœê·¼ 6ê°œì›”"),
            ("qdr:y", "ìµœê·¼ 1ë…„"),
            (None, "ì „ì²´ ê¸°ê°„"),
        ]
        log(self.__class__.__name__, f"\nSNS ê²€ìƒ‰ ì‹œìž‘ (ë³‘ë ¬ ê²€ìƒ‰)")
        log(self.__class__.__name__, f"query: {query}")
        log(self.__class__.__name__, f"time_filters: {[(tbs or 'None', name) for tbs, name in time_ranges]}")

        # YouTube ê²€ìƒ‰ì–´ ì •ì œ
        youtube_query = self._clean_youtube_query(query)
        if youtube_query != query:
            log(
                self.__class__.__name__,
                f"YouTube Query ê²€ìƒ‰ì–´ ê°•í™”: '{query}' â†’ '{youtube_query}'",
            )

        # ëª¨ë“  ì‹œê°„ ë²”ìœ„ë¥¼ ë³‘ë ¬ë¡œ ê²€ìƒ‰
        all_candidates = self._search_all_time_ranges_parallel(query, youtube_query, time_ranges)

        log(
            self.__class__.__name__,
            f"ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ - ì´ í›„ë³´: {len(all_candidates)}ê°œ",
        )

        if not all_candidates:
            log(
                self.__class__.__name__,
                f"\nSNS ê²€ìƒ‰ âŒ ìµœì¢… ê²°ê³¼: ëª¨ë“  ì‹œê°„ ë²”ìœ„ì—ì„œ SNS ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨",
            )
            return {"found": False}

        # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ (ìš°ì„ ìˆœìœ„í™”)
        sorted_candidates = self._sort_by_recency(all_candidates)
        log(
            self.__class__.__name__,
            f"ìµœì‹ ìˆœ ì •ë ¬ ì™„ë£Œ - ìš°ì„ ìˆœìœ„ í›„ë³´: {len(sorted_candidates)}ê°œ",
        )

        # ê´€ë ¨ì„± ê²€ì‚¬ (ìµœì‹  ê²ƒë¶€í„°)
        for idx, candidate in enumerate(sorted_candidates):
            url = candidate.get("url", "")
            platform = candidate.get("platform", "")
            title = candidate.get("title", "")
            thumbnail = candidate.get("thumbnail", "")
            source = candidate.get("source", "")
            time_range_priority = candidate.get("time_range_priority", "unknown")

            log(
                self.__class__.__name__,
                f"Candidate #{idx}: platform={platform}, time_range={time_range_priority}, url={url[:80] if url else 'None'}, source={source}",
            )

            if relevance_checker and user_question:
                is_relevant = relevance_checker.act(
                    user_question=user_question,
                    sns_title=title,
                    platform=platform,
                    search_term=query,
                )
                if is_relevant:
                    log(
                        self.__class__.__name__,
                        f"âœ… ê´€ë ¨ì„± í™•ì¸ ì™„ë£Œ (time_range={time_range_priority}) â†’ ê²°ê³¼ ë°˜í™˜",
                    )
                    return {
                        "found": True,
                        "platform": platform,
                        "url": url,
                        "thumbnail": thumbnail,
                        "title": title,
                    }
                else:
                    log(
                        self.__class__.__name__,
                        f"âŒ ê´€ë ¨ì„± ì—†ìŒ â†’ ë‹¤ìŒ ê²°ê³¼ íƒìƒ‰",
                    )
                    continue
            else:
                log(
                    self.__class__.__name__,
                    f"âœ… ê´€ë ¨ì„± ê²€ì¦ ì—†ì´ ê²°ê³¼ ë°˜í™˜ (time_range={time_range_priority})",
                )
                return {
                    "found": True,
                    "platform": platform,
                    "url": url,
                    "thumbnail": thumbnail,
                    "title": title,
                }

        log(
            self.__class__.__name__,
            f"\nSNS ê²€ìƒ‰ âŒ ìµœì¢… ê²°ê³¼: í›„ë³´ëŠ” ìžˆì—ˆìœ¼ë‚˜ ê´€ë ¨ì„± ìžˆëŠ” ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í•¨",
        )
        return {"found": False}

    def _clean_youtube_query(self, query: str) -> str:
        """YouTube ê²€ìƒ‰ì–´ì—ì„œ ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°"""
        youtube_query = query
        remove_keywords = [
            "ìœ íŠœë¸Œ",
            "youtube",
            "ì˜ìƒ",
            "ë™ì˜ìƒ",
            "ë¹„ë””ì˜¤",
            "video",
        ]
        for keyword in remove_keywords:
            youtube_query = (
                youtube_query.replace(keyword, "")
                .replace(keyword.upper(), "")
                .replace(keyword.capitalize(), "")
            )
        return " ".join(youtube_query.split())

    def _search_all_time_ranges_parallel(
        self, query: str, youtube_query: str, time_ranges: List[tuple]
    ) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì‹œê°„ ë²”ìœ„ë¥¼ ë³‘ë ¬ë¡œ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ ì·¨í•©"""
        all_candidates = []

        # ê° ì‹œê°„ ë²”ìœ„ë³„ë¡œ ë³‘ë ¬ ê²€ìƒ‰ íƒœìŠ¤í¬ ìƒì„±
        search_tasks = []
        for tbs_value, period_name in time_ranges:
            published_after = self._get_youtube_time_filter(tbs_value)
            search_tasks.append((query, youtube_query, tbs_value, published_after, period_name))

        # ë³‘ë ¬ ì‹¤í–‰ (max_workers=8: 4ê°œ ì‹œê°„ ë²”ìœ„ * 2ê°œ ê²€ìƒ‰ ì—”ì§„)
        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            futures = []
            for search_query, yt_query, tbs_value, published_after, period_name in search_tasks:
                # Google Images ê²€ìƒ‰
                images_future = executor.submit(
                    self._search_google_images_with_metadata,
                    search_query,
                    tbs_value,
                    period_name
                )
                futures.append(images_future)

                # YouTube Direct ê²€ìƒ‰
                youtube_future = executor.submit(
                    self._search_youtube_direct_with_metadata,
                    yt_query,
                    published_after,
                    period_name
                )
                futures.append(youtube_future)

            # ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘
            for future in concurrent.futures.as_completed(futures):
                try:
                    candidates = future.result()
                    all_candidates.extend(candidates)
                except Exception as e:
                    log(self.__class__.__name__, f"ë³‘ë ¬ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

        # ì¤‘ë³µ URL ì œê±°
        seen_urls = set()
        unique_candidates = []
        for candidate in all_candidates:
            url = candidate.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_candidates.append(candidate)

        log(
            self.__class__.__name__,
            f"ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ - ì „ì²´: {len(all_candidates)}ê°œ, ê³ ìœ : {len(unique_candidates)}ê°œ",
        )

        return unique_candidates

    def _search_google_images_with_metadata(
        self, query: str, tbs_value: Optional[str], period_name: str
    ) -> List[Dict[str, Any]]:
        """Google Images ê²€ìƒ‰ + ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        candidates = self._search_google_images(query, tbs_value)
        # ì‹œê°„ ë²”ìœ„ ìš°ì„ ìˆœìœ„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for candidate in candidates:
            candidate["time_range_priority"] = period_name
            candidate["time_range_tbs"] = tbs_value
        return candidates

    def _search_youtube_direct_with_metadata(
        self, query: str, published_after: Optional[str], period_name: str
    ) -> List[Dict[str, Any]]:
        """YouTube Direct ê²€ìƒ‰ + ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        candidates = self._search_youtube_direct(query, published_after)
        # ì‹œê°„ ë²”ìœ„ ìš°ì„ ìˆœìœ„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for candidate in candidates:
            candidate["time_range_priority"] = period_name
            candidate["time_range_published_after"] = published_after
        return candidates

    def _sort_by_recency(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í›„ë³´ë¥¼ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ (ì‹œê°„ ë²”ìœ„ ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        # ì‹œê°„ ë²”ìœ„ ìš°ì„ ìˆœìœ„ (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ìš°ì„ ìˆœìœ„)
        time_range_order = {
            "ìµœê·¼ 3ê°œì›”": 0,
            "ìµœê·¼ 6ê°œì›”": 1,
            "ìµœê·¼ 1ë…„": 2,
            "ì „ì²´ ê¸°ê°„": 3,
        }

        def get_priority(candidate: Dict[str, Any]) -> int:
            time_range = candidate.get("time_range_priority", "ì „ì²´ ê¸°ê°„")
            return time_range_order.get(time_range, 999)

        return sorted(candidates, key=get_priority)

    def extract_summary(self, search_results: Dict[str, Any]) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.
        """
        if "error" in search_results:
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {search_results['error']}"

        if "answer_box" in search_results:
            answer = search_results["answer_box"]
            if "answer" in answer:
                return f"âœ“ {answer['answer']}"
            elif "snippet" in answer:
                return f"âœ“ {answer['snippet']}"

        if "knowledge_graph" in search_results:
            kg = search_results["knowledge_graph"]
            summary = []
            if "title" in kg:
                summary.append(f"ðŸ“Œ {kg['title']}")
            if "description" in kg:
                summary.append(kg["description"])
            if summary:
                return "\n".join(summary)

        if "organic_results" in search_results and search_results["organic_results"]:
            results = []
            for result in search_results["organic_results"][:3]:
                title = result.get("title", "")
                snippet = result.get("snippet", "")
                date = result.get("date", "")

                if snippet:
                    if date:
                        results.append(f"â€¢ [{date}] {snippet}")
                    else:
                        results.append(f"â€¢ {snippet}")

            if results:
                return "\n".join(results)

        return "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def _search_google_images(
        self, query: str, tbs_value: Optional[str]
    ) -> List[Dict[str, Any]]:
        """
        Google Imagesì—ì„œ SNS ì½˜í…ì¸ ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        params = {
            "q": query,
            "api_key": self.api_key,
            "engine": "google_images",
            "num": 50,
            "hl": "ko",
            "gl": "kr",
        }
        if tbs_value:
            params["tbs"] = tbs_value

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            results = response.json()

            candidates = []
            if "images_results" in results:
                for result in results["images_results"]:
                    link = result.get("link", "")

                    if "instagram.com" in link and ("/p/" in link or "/reel/" in link):
                        candidates.append(
                            {
                                "platform": "instagram",
                                "url": link,
                                "thumbnail": result.get("thumbnail")
                                or result.get("original", ""),
                                "title": result.get("title")
                                or result.get("source", ""),
                                "source": "google_images",
                            }
                        )
                    elif "youtube.com/watch" in link or "youtu.be/" in link:
                        candidates.append(
                            {
                                "platform": "youtube",
                                "url": link,
                                "thumbnail": result.get("thumbnail")
                                or result.get("original", ""),
                                "title": result.get("title")
                                or result.get("source", ""),
                                "source": "google_images",
                            }
                        )

                    if len(candidates) >= 5:
                        break
            return candidates
        except Exception as e:
            log(self.__class__.__name__, f"Google Images ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _search_youtube_direct(
        self, query: str, published_after: Optional[str]
    ) -> List[Dict[str, Any]]:
        """
        YouTube Data API v3ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        if not self.youtube_api_key:
            log(
                self.__class__.__name__, "YouTube API í‚¤ê°€ ì—†ì–´ ì§ì ‘ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤."
            )
            return []

        params = {
            "part": "snippet",
            "q": query,
            "key": self.youtube_api_key,
            "type": "video",
            "maxResults": 5,
            "regionCode": "KR",
            "relevanceLanguage": "ko",
            "order": "relevance",
        }
        if published_after:
            params["publishedAfter"] = published_after

        try:
            log(
                self.__class__.__name__,
                f"YouTube Direct ê²€ìƒ‰ì–´: {query}, publishedAfter={published_after or 'None'}",
            )
            response = requests.get(self.youtube_base_url, params=params, timeout=30)
            response.raise_for_status()
            results = response.json()
            log(
                self.__class__.__name__,
                f"YouTube Direct API ì‘ë‹µ keys: {results.keys()}",
            )

            candidates = []
            if "items" in results:
                log(
                    self.__class__.__name__,
                    f"YouTube Direct items ê°œìˆ˜: {len(results['items'])}",
                )
                for idx, item in enumerate(results["items"]):
                    video_id = item.get("id", {}).get("videoId", "")
                    snippet = item.get("snippet", {})
                    title = snippet.get("title", "")
                    thumbnail = (
                        snippet.get("thumbnails", {}).get("high", {}).get("url", "")
                    )
                    log(
                        self.__class__.__name__,
                        f"YouTube Direct #{idx}: {title[:50]}...",
                    )
                    if video_id:
                        url = f"https://www.youtube.com/watch?v={video_id}"
                        candidates.append(
                            {
                                "platform": "youtube",
                                "url": url,
                                "thumbnail": thumbnail,
                                "title": title,
                                "source": "youtube_api_v3",
                            }
                        )
            log(
                self.__class__.__name__,
                f"YouTube Direct í›„ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(candidates)}ê°œ",
            )
            return candidates
        except Exception as e:
            log(self.__class__.__name__, f"YouTube Direct ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _get_youtube_time_filter(self, tbs_value: Optional[str]) -> Optional[str]:
        if not tbs_value:
            return None

        from datetime import datetime, timedelta

        now = datetime.utcnow()

        if tbs_value == "qdr:m3":
            published_after = now - timedelta(days=90)
        elif tbs_value == "qdr:m6":
            published_after = now - timedelta(days=180)
        elif tbs_value == "qdr:y":
            published_after = now - timedelta(days=365)
        else:
            return None

        return published_after.strftime("%Y-%m-%dT%H:%M:%SZ")
